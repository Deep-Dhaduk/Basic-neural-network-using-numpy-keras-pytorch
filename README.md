# 3-Layer Deep Neural Network for Non-Linear Regression

## Using NumPy, PyTorch, PyTorch Lightning, and TensorFlow

---

## ðŸ“‹ Project Overview

This repository implements a **3-layer deep neural network** for **non-linear regression with 3 input variables** using multiple frameworks. Each Colab notebook demonstrates a different approach â€” from pure NumPy with manual backpropagation to high-level TensorFlow/Keras APIs.

### Target Non-Linear Equation

$$y = \sin(x_1) \cdot x_2^2 + \cos(x_3) \cdot x_1 + x_2 \cdot x_3^2$$

### Network Architecture (All Notebooks)

```
Input(3) â†’ Dense(64, ReLU) â†’ Dense(32, ReLU) â†’ Dense(16, ReLU) â†’ Dense(1, Linear)
```

- **3 hidden layers** (64 â†’ 32 â†’ 16 neurons)
- **ReLU** activation for hidden layers
- **He/Kaiming** weight initialization
- **MSE** loss function
- **4D visualization** using PCA dimensionality reduction (scikit-learn)

---

## ðŸ“‚ File Structure

Each notebook lives in its own folder along with a **README.md** and a **video walkthrough** explaining the Colab.

```
â”œâ”€â”€ README.md                                          â† This file
â”œâ”€â”€ Colab_A_NumPy_3Layer_NN.ipynb/
â”‚   â”œâ”€â”€ Colab_A_NumPy_3Layer_NN.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ðŸŽ¥ Video walkthrough
â”œâ”€â”€ Colab_B_PyTorch_Scratch_3Layer_NN.ipynb/
â”‚   â”œâ”€â”€ Colab_B_PyTorch_Scratch_3Layer_NN.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ðŸŽ¥ Video walkthrough
â”œâ”€â”€ Colab_C_PyTorch_Classes_3Layer_NN.ipynb/
â”‚   â”œâ”€â”€ Colab_C_PyTorch_Classes_3Layer_NN.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ðŸŽ¥ Video walkthrough
â”œâ”€â”€ Colab_D_PyTorch_Lightning_3Layer_NN.ipynb/
â”‚   â”œâ”€â”€ Colab_D_PyTorch_Lightning_3Layer_NN.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ðŸŽ¥ Video walkthrough
â”œâ”€â”€ Colab_Ei_TF_Scratch_3Layer_NN/
â”‚   â”œâ”€â”€ Colab_Ei_TF_Scratch_3Layer_NN.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ðŸŽ¥ Video walkthrough
â”œâ”€â”€ Colab_Eii_TF_BuiltInLayers_3Layer_NN/
â”‚   â”œâ”€â”€ Colab_Eii_TF_BuiltInLayers_3Layer_NN.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ðŸŽ¥ Video walkthrough
â”œâ”€â”€ Colab_Eiii_TF_FunctionalAPI_3Layer_NN/
â”‚   â”œâ”€â”€ Colab_Eiii_TF_FunctionalAPI_3Layer_NN.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ðŸŽ¥ Video walkthrough
â””â”€â”€ Colab_Eiv_TF_HighLevelAPI_3Layer_NN/
    â”œâ”€â”€ Colab_Eiv_TF_HighLevelAPI_3Layer_NN.ipynb
    â”œâ”€â”€ README.md
    â””â”€â”€ ðŸŽ¥ Video walkthrough
```

| Folder | Framework | Description |
|--------|-----------|-------------|
| `Colab_A_NumPy_3Layer_NN.ipynb/` | NumPy + tf.einsum | From-scratch NN with manual backprop & chain rule |
| `Colab_B_PyTorch_Scratch_3Layer_NN.ipynb/` | PyTorch (raw tensors) | From-scratch NN WITHOUT built-in layers |
| `Colab_C_PyTorch_Classes_3Layer_NN.ipynb/` | PyTorch (nn.Module) | Class-based NN using built-in PyTorch modules |
| `Colab_D_PyTorch_Lightning_3Layer_NN.ipynb/` | PyTorch Lightning | Lightning framework with DataModule & callbacks |
| `Colab_Ei_TF_Scratch_3Layer_NN/` | TensorFlow (low-level) | tf.Variable + tf.GradientTape + tf.einsum |
| `Colab_Eii_TF_BuiltInLayers_3Layer_NN/` | TensorFlow (Dense layers) | keras.layers.Dense + custom GradientTape loop |
| `Colab_Eiii_TF_FunctionalAPI_3Layer_NN/` | TensorFlow (Functional) | Functional API with Input/Output graph |
| `Colab_Eiv_TF_HighLevelAPI_3Layer_NN/` | TensorFlow (Sequential) | Highest-level API with model.fit() |

---

## ðŸŽ¥ Video Walkthroughs

> **Each notebook has a corresponding video code walkthrough uploaded in the same folder, explaining every section of the Colab.** Each folder also contains its own README with detailed descriptions.

| Colab | Video Location | Folder README |
|-------|---------------|---------------|
| **Colab A** - NumPy from Scratch | ðŸ“¹ Video in `Colab_A_NumPy_3Layer_NN.ipynb/` | [README](Colab_A_NumPy_3Layer_NN.ipynb/README.md) |
| **Colab B** - PyTorch from Scratch | ðŸ“¹ Video in `Colab_B_PyTorch_Scratch_3Layer_NN.ipynb/` | [README](Colab_B_PyTorch_Scratch_3Layer_NN.ipynb/README.md) |
| **Colab C** - PyTorch Classes | ðŸ“¹ Video in `Colab_C_PyTorch_Classes_3Layer_NN.ipynb/` | [README](Colab_C_PyTorch_Classes_3Layer_NN.ipynb/README.md) |
| **Colab D** - PyTorch Lightning | ðŸ“¹ Video in `Colab_D_PyTorch_Lightning_3Layer_NN.ipynb/` | [README](Colab_D_PyTorch_Lightning_3Layer_NN.ipynb/README.md) |
| **Colab E-i** - TF from Scratch | ðŸ“¹ Video in `Colab_Ei_TF_Scratch_3Layer_NN/` | [README](Colab_Ei_TF_Scratch_3Layer_NN/README.md) |
| **Colab E-ii** - TF Built-in Layers | ðŸ“¹ Video in `Colab_Eii_TF_BuiltInLayers_3Layer_NN/` | [README](Colab_Eii_TF_BuiltInLayers_3Layer_NN/README.md) |
| **Colab E-iii** - TF Functional API | ðŸ“¹ Video in `Colab_Eiii_TF_FunctionalAPI_3Layer_NN/` | [README](Colab_Eiii_TF_FunctionalAPI_3Layer_NN/README.md) |
| **Colab E-iv** - TF High-Level API | ðŸ“¹ Video in `Colab_Eiv_TF_HighLevelAPI_3Layer_NN/` | [README](Colab_Eiv_TF_HighLevelAPI_3Layer_NN/README.md) |

---

## ðŸ““ Detailed Notebook Descriptions

### Colab A: NumPy-Only 3-Layer DNN (`Colab_A_NumPy_3Layer_NN.ipynb`)

**Framework:** NumPy (with `tf.einsum` for matrix multiplication)

**Key Features:**
- Pure NumPy implementation â€” no framework autograd
- **`tf.einsum('ij,jk->ik', A, B)`** used instead of `np.dot` or `@` for all matrix multiplications
- **Manual forward pass** through 3 hidden layers + output
- **Manual backpropagation** implementing chain rule gradient propagation:
  - `dL/dW4 = A3áµ€ @ dZ4`
  - `dL/dW3 = A2áµ€ @ (dZ4 @ W4áµ€ âŠ™ ReLU'(Z3))`
  - `dL/dW2 = A1áµ€ @ (... chain continues ...)`
  - `dL/dW1 = Xáµ€ @ (... chain continues further ...)`
- He initialization for weights
- Mini-batch gradient descent
- 4D visualization with PCA (scikit-learn)

**Sections:**
1. Imports & setup
2. Synthetic data generation (3 variables)
3. 4D plotting with PCA dimensionality reduction
4. Weight initialization & architecture definition
5. Activation functions and derivatives (ReLU, Linear)
6. Forward pass using `tf.einsum`
7. MSE loss function & derivative
8. Backward pass â€” full manual chain rule backpropagation
9. Training loop with mini-batch gradient descent
10. Training visualization (loss curves, predictions, residuals)
11. Sample predictions table (denormalized)
12. 4D prediction comparison plots

---

### Colab B: PyTorch From Scratch (`Colab_B_PyTorch_Scratch_3Layer_NN.ipynb`)

**Framework:** PyTorch (raw tensors only)

**Key Features:**
- **NO `nn.Module`**, NO `nn.Linear`, NO `nn.functional`
- Raw `torch.Tensor` with `requires_grad=True`
- Matrix multiplication via `torch.mm()`
- ReLU implemented as `torch.clamp(Z, min=0)`
- **NO optimizer** â€” manual `p -= lr * p.grad` updates
- PyTorch autograd computes gradients, but weight updates are manual

**What's NOT used:**  `nn.Module`, `nn.Linear`, `nn.ReLU`, `optim.Adam`, `optim.SGD`, `DataLoader`

---

### Colab C: PyTorch Class-Based (`Colab_C_PyTorch_Classes_3Layer_NN.ipynb`)

**Framework:** PyTorch (full nn.Module)

**Key Features:**
- `ThreeLayerDNN(nn.Module)` class with `__init__` and `forward`
- Uses `nn.Linear`, `nn.ReLU` built-in layers
- Kaiming/He initialization via `nn.init.kaiming_normal_`
- `Adam` optimizer with `ReduceLROnPlateau` scheduler
- `DataLoader` for mini-batch training
- `model.train()` / `model.eval()` mode switching
- `loss.backward()` â†’ `optimizer.step()` training loop

---

### Colab D: PyTorch Lightning (`Colab_D_PyTorch_Lightning_3Layer_NN.ipynb`)

**Framework:** PyTorch Lightning

**Key Features:**
- `ThreeLayerLightningDNN(pl.LightningModule)` â€” encapsulates model + training
- `NonLinearRegressionDataModule(pl.LightningDataModule)` â€” encapsulates data pipeline
- `training_step()`, `validation_step()`, `configure_optimizers()` methods
- `pl.Trainer` with callbacks:
  - `EarlyStopping` (patience=100)
  - `ModelCheckpoint` (save best model)
- Automatic logging, progress bars, device management
- `ReduceLROnPlateau` scheduler configured in `configure_optimizers()`

---

### Colab E-i: TensorFlow From Scratch (`Colab_Ei_TF_Scratch_3Layer_NN.ipynb`)

**Framework:** TensorFlow (low-level only)

**Key Features:**
- **NO `tf.keras`** layers â€” pure `tf.Variable` weights
- `tf.einsum('ij,jk->ik', X, W)` for all matrix multiplications
- `tf.nn.relu()` for activation (NOT keras activation)
- `tf.GradientTape()` for automatic differentiation
- Manual training loop â€” **NO `model.fit()`**
- `@tf.function` decorator for compiled execution
- `tf.data.Dataset` for batching

---

### Colab E-ii: TensorFlow with Built-in Layers (`Colab_Eii_TF_BuiltInLayers_3Layer_NN.ipynb`)

**Framework:** TensorFlow (keras.layers.Dense + custom loop)

**Key Features:**
- `layers.Dense(64, activation='relu', kernel_initializer='he_normal')` for layer creation
- Layers called explicitly in a `forward()` function
- Still uses `tf.GradientTape()` custom training loop
- **NOT using `model.fit()`** â€” manual loop with `optimizer.apply_gradients()`
- `tf.keras.losses.MeanSquaredError()` for loss

---

### Colab E-iii: TensorFlow Functional API (`Colab_Eiii_TF_FunctionalAPI_3Layer_NN.ipynb`)

**Framework:** TensorFlow Functional API

**Key Features:**
- `Input(shape=(3,))` â†’ `Dense(...)()` â†’ `Model(inputs, outputs)` pattern
- DAG-based model definition (supports complex topologies)
- `model.compile(optimizer, loss, metrics)` for configuration
- `model.fit()` for training with validation
- Callbacks: `EarlyStopping`, `ReduceLROnPlateau`
- `history` object for loss/metric tracking
- Model visualization with `plot_model()`

---

### Colab E-iv: TensorFlow High-Level API (`Colab_Eiv_TF_HighLevelAPI_3Layer_NN.ipynb`)

**Framework:** TensorFlow/Keras Sequential API

**Key Features:**
- `Sequential([Dense, BatchNorm, Dense, BatchNorm, ...])` â€” simplest API
- `BatchNormalization` for training stability
- Full `model.compile()` â†’ `model.fit()` â†’ `model.evaluate()` â†’ `model.predict()` pipeline
- Multiple callbacks: `EarlyStopping`, `ReduceLROnPlateau`, `ModelCheckpoint`
- `train_test_split` from scikit-learn
- 4D prediction comparison visualization
- Most concise implementation

---

## ðŸ”„ Framework Comparison

| Feature | Colab A | Colab B | Colab C | Colab D | E-i | E-ii | E-iii | E-iv |
|---------|---------|---------|---------|---------|-----|------|-------|------|
| **Framework** | NumPy | PyTorch | PyTorch | PL | TF | TF | TF | TF |
| **Abstraction Level** | Lowest | Low | Medium | High | Lowest | Medium | High | Highest |
| **Manual Backprop** | âœ… | Autograd | Autograd | Autograd | GradTape | GradTape | model.fit | model.fit |
| **Built-in Layers** | âŒ | âŒ | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… |
| **Optimizer** | Manual SGD | Manual | Adam | Adam | Adam | Adam | Adam | Adam |
| **tf.einsum** | âœ… | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ |
| **BatchNorm** | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| **Callbacks** | âŒ | âŒ | LR Sched | ES+Ckpt | âŒ | âŒ | ES+LR | ES+LR+Ckpt |

---

## ðŸš€ How to Run

### Option 1: Google Colab (Recommended)
1. Upload any `.ipynb` file to [Google Colab](https://colab.research.google.com/)
2. Click **Runtime â†’ Run All**
3. All dependencies are pre-installed on Colab (except `pytorch-lightning` which is installed in Colab D)

### Option 2: Local Jupyter
```bash
pip install numpy tensorflow torch pytorch-lightning matplotlib scikit-learn
jupyter notebook
```

### Option 3: Open from GitHub
Click the "Open in Colab" badge for each notebook (after pushing to GitHub).

---

## ðŸ“Š Expected Results

All notebooks should achieve:
- **RÂ² > 0.95** on the test set
- **Loss convergence** visible in training curves
- **Residuals** approximately normally distributed around 0

---

## ðŸ§® Mathematical Background

### Forward Pass (Layer l)
$$Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$$
$$A^{[l]} = \text{ReLU}(Z^{[l]}) = \max(0, Z^{[l]})$$

### Backpropagation Chain Rule
$$\frac{\partial L}{\partial W^{[l]}} = (A^{[l-1]})^T \cdot \delta^{[l]}$$

where:
$$\delta^{[L]} = \frac{\partial L}{\partial A^{[L]}} \odot \sigma'(Z^{[L]})$$
$$\delta^{[l]} = (\delta^{[l+1]} \cdot (W^{[l+1]})^T) \odot \sigma'(Z^{[l]})$$

### tf.einsum Usage (Colabs A & E-i)
```python
# Instead of: np.dot(X, W) or X @ W
# We use:
tf.einsum('ij,jk->ik', X, W)  # Matrix multiplication
tf.einsum('ji,jk->ik', A, dZ)  # A^T @ dZ (transpose first matrix)
```

---

## ðŸ“¦ Dependencies

| Package | Version | Used In |
|---------|---------|---------|
| `numpy` | â‰¥1.21 | All |
| `tensorflow` | â‰¥2.10 | A, E-i, E-ii, E-iii, E-iv |
| `torch` | â‰¥1.12 | B, C, D |
| `pytorch-lightning` | â‰¥2.0 | D |
| `matplotlib` | â‰¥3.5 | All |
| `scikit-learn` | â‰¥1.0 | All (PCA, train_test_split) |

---

## ðŸ‘¤ Author

[Your Name]

## ðŸ“„ License

This project is for educational purposes.
