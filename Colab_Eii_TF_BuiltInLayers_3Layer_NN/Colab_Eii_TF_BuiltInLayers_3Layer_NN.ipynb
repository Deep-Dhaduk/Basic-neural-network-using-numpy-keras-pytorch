{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f015af",
   "metadata": {},
   "source": [
    "# Colab E-ii: TensorFlow with Built-in Layers - 3-Layer Deep Neural Network\n",
    "\n",
    "## Overview\n",
    "- **TensorFlow `tf.keras.layers`** for layer definitions\n",
    "- Uses `tf.keras.layers.Dense` with custom training loop via `tf.GradientTape`\n",
    "- NOT using `model.fit()` - still manual training loop\n",
    "- Same 3-variable non-linear regression problem\n",
    "\n",
    "### Target Non-Linear Equation\n",
    "$$y = \\sin(x_1) \\cdot x_2^2 + \\cos(x_3) \\cdot x_1 + x_2 \\cdot x_3^2$$\n",
    "\n",
    "### Network Architecture\n",
    "- Input Layer: 3 neurons\n",
    "- Hidden Layer 1: 64 neurons (ReLU) - `Dense(64, activation='relu')`\n",
    "- Hidden Layer 2: 32 neurons (ReLU) - `Dense(32, activation='relu')`\n",
    "- Hidden Layer 3: 16 neurons (ReLU) - `Dense(16, activation='relu')`\n",
    "- Output Layer: 1 neuron (Linear) - `Dense(1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 1: Imports\n",
    "# ============================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a74830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 2: Generate Synthetic Data\n",
    "# ============================================================\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "x1 = np.random.uniform(-2, 2, (N_SAMPLES, 1)).astype(np.float32)\n",
    "x2 = np.random.uniform(-2, 2, (N_SAMPLES, 1)).astype(np.float32)\n",
    "x3 = np.random.uniform(-2, 2, (N_SAMPLES, 1)).astype(np.float32)\n",
    "\n",
    "y = (np.sin(x1) * x2**2 + np.cos(x3) * x1 + x2 * x3**2).astype(np.float32)\n",
    "X = np.hstack([x1, x2, x3])\n",
    "\n",
    "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
    "y_mean, y_std = y.mean(), y.std()\n",
    "X_norm = ((X - X_mean) / X_std).astype(np.float32)\n",
    "y_norm = ((y - y_mean) / y_std).astype(np.float32)\n",
    "\n",
    "split = int(0.8 * N_SAMPLES)\n",
    "X_train, X_test = X_norm[:split], X_norm[split:]\n",
    "y_train, y_test = y_norm[:split], y_norm[split:]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(BATCH_SIZE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 3: 4D Data Visualization\n",
    "# ============================================================\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "sc1 = ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=y.flatten(), cmap='viridis', s=5, alpha=0.6)\n",
    "ax1.set_xlabel('x1'); ax1.set_ylabel('x2'); ax1.set_zlabel('x3')\n",
    "ax1.set_title('4D: x1,x2,x3 (color=y)')\n",
    "plt.colorbar(sc1, ax=ax1, shrink=0.5)\n",
    "\n",
    "pca2 = PCA(n_components=2); Xp2 = pca2.fit_transform(X)\n",
    "ax2 = fig.add_subplot(132)\n",
    "sc2 = ax2.scatter(Xp2[:, 0], Xp2[:, 1], c=y.flatten(), cmap='viridis', s=5, alpha=0.6)\n",
    "ax2.set_xlabel('PC1'); ax2.set_ylabel('PC2'); ax2.set_title('PCA 2D')\n",
    "plt.colorbar(sc2, ax=ax2, shrink=0.5)\n",
    "\n",
    "pca3 = PCA(n_components=3); Xp3 = pca3.fit_transform(X)\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "sc3 = ax3.scatter(Xp3[:, 0], Xp3[:, 1], Xp3[:, 2], c=y.flatten(), cmap='viridis', s=5, alpha=0.6)\n",
    "ax3.set_xlabel('PC1'); ax3.set_ylabel('PC2'); ax3.set_zlabel('PC3')\n",
    "ax3.set_title('PCA 3D (4D)')\n",
    "plt.colorbar(sc3, ax=ax3, shrink=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 4: Build Model Using tf.keras.layers.Dense\n",
    "# ============================================================\n",
    "# Using built-in Dense layers but with CUSTOM training loop\n",
    "\n",
    "# Define layers individually (not using Sequential yet - just layers)\n",
    "dense1 = layers.Dense(64, activation='relu',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='hidden_1')\n",
    "dense2 = layers.Dense(32, activation='relu',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='hidden_2')\n",
    "dense3 = layers.Dense(16, activation='relu',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='hidden_3')\n",
    "output_layer = layers.Dense(1, name='output')\n",
    "\n",
    "# Build layers by passing dummy input\n",
    "dummy = tf.zeros((1, 3))\n",
    "_ = output_layer(dense3(dense2(dense1(dummy))))\n",
    "\n",
    "# Collect all trainable variables\n",
    "all_layers = [dense1, dense2, dense3, output_layer]\n",
    "all_variables = []\n",
    "for layer in all_layers:\n",
    "    all_variables.extend(layer.trainable_variables)\n",
    "\n",
    "print(\"Built-in Layers (Dense):\")\n",
    "for layer in all_layers:\n",
    "    w = layer.trainable_variables\n",
    "    n = sum(v.numpy().size for v in w)\n",
    "    print(f\"  {layer.name}: {[v.shape.as_list() for v in w]} ({n} params)\")\n",
    "\n",
    "total = sum(v.numpy().size for v in all_variables)\n",
    "print(f\"Total parameters: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8baf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 5: Forward Pass Function\n",
    "# ============================================================\n",
    "\n",
    "@tf.function\n",
    "def forward(X):\n",
    "    \"\"\"Forward pass through built-in Dense layers.\"\"\"\n",
    "    h1 = dense1(X)\n",
    "    h2 = dense2(h1)\n",
    "    h3 = dense3(h2)\n",
    "    out = output_layer(h3)\n",
    "    return out\n",
    "\n",
    "# Test\n",
    "test_out = forward(tf.constant(X_train[:5]))\n",
    "print(f\"Forward test: {test_out.numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a11075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 6: Custom Training Loop with GradientTape\n",
    "# ============================================================\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 500\n",
    "optimizer = tf.optimizers.Adam(LEARNING_RATE)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(X_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = forward(X_batch)\n",
    "        loss = loss_fn(y_batch, y_pred)\n",
    "    grads = tape.gradient(loss, all_variables)\n",
    "    optimizer.apply_gradients(zip(grads, all_variables))\n",
    "    return loss\n",
    "\n",
    "print(f\"Custom training loop with Dense layers + GradientTape\")\n",
    "print(f\"LR: {LEARNING_RATE}, Epochs: {EPOCHS}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    ep_loss = 0.0\n",
    "    nb = 0\n",
    "    for xb, yb in train_ds:\n",
    "        loss = train_step(xb, yb)\n",
    "        ep_loss += loss.numpy()\n",
    "        nb += 1\n",
    "    \n",
    "    avg_train = ep_loss / nb\n",
    "    train_losses.append(avg_train)\n",
    "    \n",
    "    y_tp = forward(tf.constant(X_test))\n",
    "    t_loss = loss_fn(y_test, y_tp).numpy()\n",
    "    test_losses.append(t_loss)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:4d}/{EPOCHS}] | Train: {avg_train:.6f} | Test: {t_loss:.6f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final Train: {train_losses[-1]:.6f}, Test: {test_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 7: Results Visualization\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train', alpha=0.8)\n",
    "axes[0].plot(test_losses, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('Loss Curves'); axes[0].legend()\n",
    "axes[0].set_yscale('log'); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "y_final = forward(tf.constant(X_test)).numpy()\n",
    "\n",
    "axes[1].scatter(y_test, y_final, alpha=0.5, s=10)\n",
    "mn, mx = min(y_test.min(), y_final.min()), max(y_test.max(), y_final.max())\n",
    "axes[1].plot([mn, mx], [mn, mx], 'r--', lw=2, label='Perfect')\n",
    "axes[1].set_xlabel('Actual'); axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title('Pred vs Actual'); axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "res = (y_test - y_final).flatten()\n",
    "axes[2].hist(res, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(0, color='r', linestyle='--')\n",
    "axes[2].set_xlabel('Residual'); axes[2].set_ylabel('Count')\n",
    "axes[2].set_title(f'Residuals (std={res.std():.4f})')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "r2 = 1 - np.sum(res**2) / np.sum((y_test.flatten() - y_test.mean())**2)\n",
    "print(f\"R²: {r2:.6f}, MAE: {np.mean(np.abs(res)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 8: Sample Predictions\n",
    "# ============================================================\n",
    "y_to = y_test * y_std + y_mean\n",
    "y_po = y_final * y_std + y_mean\n",
    "X_to = X_test * X_std + X_mean\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Idx':>5} | {'x1':>7} | {'x2':>7} | {'x3':>7} | {'Actual':>9} | {'Pred':>9} | {'Err':>7}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(15):\n",
    "    a, p = y_to[i,0], y_po[i,0]\n",
    "    print(f\"{i:5d} | {X_to[i,0]:7.3f} | {X_to[i,1]:7.3f} | {X_to[i,2]:7.3f} | {a:9.4f} | {p:9.4f} | {abs(a-p):7.4f}\")\n",
    "\n",
    "print(f\"\\nRMSE: {np.sqrt(np.mean((y_to-y_po)**2)):.4f}, R²: {r2:.6f}\")\n",
    "\n",
    "print(\"\\n=== Colab E-ii Complete ===\")\n",
    "print(\"Key: TF built-in Dense layers + custom training loop\")\n",
    "print(\"- tf.keras.layers.Dense for layer definitions\")\n",
    "print(\"- He (Kaiming) initialization via kernel_initializer\")\n",
    "print(\"- tf.GradientTape for manual gradient computation\")\n",
    "print(\"- Adam optimizer for weight updates\")\n",
    "print(\"- NOT using model.fit() - custom loop\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
