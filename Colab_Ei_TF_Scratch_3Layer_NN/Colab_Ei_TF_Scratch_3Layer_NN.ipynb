{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34b9e62",
   "metadata": {},
   "source": [
    "# Colab E-i: TensorFlow From Scratch - 3-Layer Deep Neural Network\n",
    "\n",
    "## Overview\n",
    "- **TensorFlow low-level API only** - no `tf.keras`, no high-level layers\n",
    "- Manual `tf.Variable` for weights, `tf.GradientTape` for backprop\n",
    "- Uses `tf.einsum` for matrix multiplication\n",
    "- Same 3-variable non-linear regression problem\n",
    "\n",
    "### Target Non-Linear Equation\n",
    "$$y = \\sin(x_1) \\cdot x_2^2 + \\cos(x_3) \\cdot x_1 + x_2 \\cdot x_3^2$$\n",
    "\n",
    "### Network Architecture\n",
    "- Input Layer: 3 neurons\n",
    "- Hidden Layer 1: 64 neurons (ReLU)\n",
    "- Hidden Layer 2: 32 neurons (ReLU)\n",
    "- Hidden Layer 3: 16 neurons (ReLU)\n",
    "- Output Layer: 1 neuron (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cacdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 1: Imports\n",
    "# ============================================================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec37f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 2: Generate Synthetic Data\n",
    "# ============================================================\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "x1 = np.random.uniform(-2, 2, (N_SAMPLES, 1)).astype(np.float32)\n",
    "x2 = np.random.uniform(-2, 2, (N_SAMPLES, 1)).astype(np.float32)\n",
    "x3 = np.random.uniform(-2, 2, (N_SAMPLES, 1)).astype(np.float32)\n",
    "\n",
    "y = (np.sin(x1) * x2**2 + np.cos(x3) * x1 + x2 * x3**2).astype(np.float32)\n",
    "X = np.hstack([x1, x2, x3])\n",
    "\n",
    "# Normalize\n",
    "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
    "y_mean, y_std = y.mean(), y.std()\n",
    "X_norm = ((X - X_mean) / X_std).astype(np.float32)\n",
    "y_norm = ((y - y_mean) / y_std).astype(np.float32)\n",
    "\n",
    "# Split\n",
    "split = int(0.8 * N_SAMPLES)\n",
    "X_train, X_test = X_norm[:split], X_norm[split:]\n",
    "y_train, y_test = y_norm[:split], y_norm[split:]\n",
    "\n",
    "# Convert to TF tensors\n",
    "X_train_tf = tf.constant(X_train)\n",
    "y_train_tf = tf.constant(y_train)\n",
    "X_test_tf = tf.constant(X_test)\n",
    "y_test_tf = tf.constant(y_test)\n",
    "\n",
    "# Create TF Dataset\n",
    "BATCH_SIZE = 64\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf)).shuffle(1000).batch(BATCH_SIZE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test_tf, y_test_tf)).batch(BATCH_SIZE)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 3: 4D Data Visualization\n",
    "# ============================================================\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "sc1 = ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=y.flatten(), cmap='viridis', s=5, alpha=0.6)\n",
    "ax1.set_xlabel('x1'); ax1.set_ylabel('x2'); ax1.set_zlabel('x3')\n",
    "ax1.set_title('4D: x1,x2,x3 (color=y)')\n",
    "plt.colorbar(sc1, ax=ax1, shrink=0.5)\n",
    "\n",
    "pca2 = PCA(n_components=2); Xp2 = pca2.fit_transform(X)\n",
    "ax2 = fig.add_subplot(132)\n",
    "sc2 = ax2.scatter(Xp2[:, 0], Xp2[:, 1], c=y.flatten(), cmap='viridis', s=5, alpha=0.6)\n",
    "ax2.set_xlabel('PC1'); ax2.set_ylabel('PC2')\n",
    "ax2.set_title('PCA 2D (color=y)')\n",
    "plt.colorbar(sc2, ax=ax2, shrink=0.5)\n",
    "\n",
    "pca3 = PCA(n_components=3); Xp3 = pca3.fit_transform(X)\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "sc3 = ax3.scatter(Xp3[:, 0], Xp3[:, 1], Xp3[:, 2], c=y.flatten(), cmap='viridis', s=5, alpha=0.6)\n",
    "ax3.set_xlabel('PC1'); ax3.set_ylabel('PC2'); ax3.set_zlabel('PC3')\n",
    "ax3.set_title('PCA 3D + color=y (4D)')\n",
    "plt.colorbar(sc3, ax=ax3, shrink=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df32c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 4: Initialize Weights as tf.Variable (Low-Level)\n",
    "# ============================================================\n",
    "# NO keras layers! Pure tf.Variable weights.\n",
    "\n",
    "def he_init_tf(fan_in, fan_out):\n",
    "    \"\"\"He initialization using tf.Variable.\"\"\"\n",
    "    std = tf.sqrt(2.0 / tf.cast(fan_in, tf.float32))\n",
    "    init = tf.random.normal([fan_in, fan_out], stddev=std)\n",
    "    return tf.Variable(init, trainable=True, name=f'W_{fan_in}x{fan_out}')\n",
    "\n",
    "def zero_bias_tf(size):\n",
    "    return tf.Variable(tf.zeros([1, size]), trainable=True, name=f'b_{size}')\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Architecture: 3 -> 64 -> 32 -> 16 -> 1\n",
    "W1 = he_init_tf(3, 64);   b1 = zero_bias_tf(64)\n",
    "W2 = he_init_tf(64, 32);  b2 = zero_bias_tf(32)\n",
    "W3 = he_init_tf(32, 16);  b3 = zero_bias_tf(16)\n",
    "W4 = he_init_tf(16, 1);   b4 = zero_bias_tf(1)\n",
    "\n",
    "all_variables = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "total_params = sum(v.numpy().size for v in all_variables)\n",
    "print(f\"Network (pure tf.Variable, NO keras):\")\n",
    "for v in all_variables:\n",
    "    print(f\"  {v.name}: {v.shape}\")\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c712ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 5: Forward Pass with tf.einsum (Low-Level)\n",
    "# ============================================================\n",
    "\n",
    "@tf.function  # Compile for performance\n",
    "def forward(X, W1, b1, W2, b2, W3, b3, W4, b4):\n",
    "    \"\"\"\n",
    "    Low-level forward pass using tf.einsum for matrix multiplication.\n",
    "    No keras.layers, no keras.activations!\n",
    "    \"\"\"\n",
    "    # Hidden Layer 1: Z = X @ W + b, A = ReLU(Z)\n",
    "    Z1 = tf.einsum('ij,jk->ik', X, W1) + b1\n",
    "    A1 = tf.nn.relu(Z1)  # Low-level tf.nn.relu (not keras)\n",
    "    \n",
    "    # Hidden Layer 2\n",
    "    Z2 = tf.einsum('ij,jk->ik', A1, W2) + b2\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "    # Hidden Layer 3\n",
    "    Z3 = tf.einsum('ij,jk->ik', A2, W3) + b3\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    \n",
    "    # Output Layer (linear)\n",
    "    Z4 = tf.einsum('ij,jk->ik', A3, W4) + b4\n",
    "    return Z4\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(y_true, y_pred):\n",
    "    \"\"\"MSE loss from scratch.\"\"\"\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "# Test\n",
    "test_pred = forward(X_train_tf[:5], W1, b1, W2, b2, W3, b3, W4, b4)\n",
    "print(f\"Forward test output: {test_pred.numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 6: Training with tf.GradientTape (Low-Level)\n",
    "# ============================================================\n",
    "# Manual gradient computation using GradientTape - NO keras model.fit!\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 500\n",
    "\n",
    "# Manual optimizer (low-level SGD with momentum-like behavior)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Re-init weights\n",
    "tf.random.set_seed(42)\n",
    "W1 = he_init_tf(3, 64);   b1 = zero_bias_tf(64)\n",
    "W2 = he_init_tf(64, 32);  b2 = zero_bias_tf(32)\n",
    "W3 = he_init_tf(32, 16);  b3 = zero_bias_tf(16)\n",
    "W4 = he_init_tf(16, 1);   b4 = zero_bias_tf(1)\n",
    "all_variables = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(X_batch, y_batch):\n",
    "    \"\"\"Single training step with GradientTape.\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = forward(X_batch, W1, b1, W2, b2, W3, b3, W4, b4)\n",
    "        loss = compute_loss(y_batch, y_pred)\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, all_variables)\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, all_variables))\n",
    "    return loss\n",
    "\n",
    "print(f\"Training with tf.GradientTape (NO keras.fit!)\")\n",
    "print(f\"LR: {LEARNING_RATE}, Epochs: {EPOCHS}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_ds:\n",
    "        loss = train_step(X_batch, y_batch)\n",
    "        epoch_loss += loss.numpy()\n",
    "        n_batches += 1\n",
    "    \n",
    "    avg_train = epoch_loss / n_batches\n",
    "    train_losses.append(avg_train)\n",
    "    \n",
    "    # Test loss\n",
    "    y_test_pred = forward(X_test_tf, W1, b1, W2, b2, W3, b3, W4, b4)\n",
    "    t_loss = compute_loss(y_test_tf, y_test_pred).numpy()\n",
    "    test_losses.append(t_loss)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:4d}/{EPOCHS}] | Train: {avg_train:.6f} | Test: {t_loss:.6f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final Train: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final Test:  {test_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 7: Results Visualization\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(train_losses, label='Train', alpha=0.8)\n",
    "axes[0].plot(test_losses, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('Loss Curves'); axes[0].legend()\n",
    "axes[0].set_yscale('log'); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "y_final = forward(X_test_tf, W1, b1, W2, b2, W3, b3, W4, b4).numpy()\n",
    "y_act = y_test\n",
    "\n",
    "axes[1].scatter(y_act, y_final, alpha=0.5, s=10)\n",
    "mn, mx = min(y_act.min(), y_final.min()), max(y_act.max(), y_final.max())\n",
    "axes[1].plot([mn, mx], [mn, mx], 'r--', lw=2, label='Perfect')\n",
    "axes[1].set_xlabel('Actual'); axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title('Pred vs Actual'); axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "res = (y_act - y_final).flatten()\n",
    "axes[2].hist(res, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(0, color='r', linestyle='--')\n",
    "axes[2].set_xlabel('Residual'); axes[2].set_ylabel('Count')\n",
    "axes[2].set_title(f'Residuals (std={res.std():.4f})')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "r2 = 1 - np.sum(res**2) / np.sum((y_act.flatten() - y_act.mean())**2)\n",
    "print(f\"R²: {r2:.6f}, MAE: {np.mean(np.abs(res)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69412c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 8: Sample Predictions\n",
    "# ============================================================\n",
    "y_test_orig = y_act * y_std + y_mean\n",
    "y_pred_orig = y_final * y_std + y_mean\n",
    "X_test_orig = X_test * X_std + X_mean\n",
    "\n",
    "print(\"Sample Predictions (Original Scale):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Idx':>5} | {'x1':>7} | {'x2':>7} | {'x3':>7} | {'Actual':>9} | {'Predicted':>9} | {'Error':>7}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(15):\n",
    "    a, p = y_test_orig[i, 0], y_pred_orig[i, 0]\n",
    "    print(f\"{i:5d} | {X_test_orig[i,0]:7.3f} | {X_test_orig[i,1]:7.3f} | {X_test_orig[i,2]:7.3f} | {a:9.4f} | {p:9.4f} | {abs(a-p):7.4f}\")\n",
    "\n",
    "print(f\"\\nRMSE: {np.sqrt(np.mean((y_test_orig-y_pred_orig)**2)):.4f}\")\n",
    "print(f\"R²: {r2:.6f}\")\n",
    "\n",
    "print(\"\\n=== Colab E-i Complete ===\")\n",
    "print(\"Key: Pure TF low-level API\")\n",
    "print(\"- tf.Variable for weights (NO keras layers)\")\n",
    "print(\"- tf.einsum for matrix multiplication\")\n",
    "print(\"- tf.GradientTape for backpropagation\")\n",
    "print(\"- tf.nn.relu for activation (NOT keras)\")\n",
    "print(\"- Manual training loop (NO model.fit)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
